{
 "metadata": {
  "name": "Translation Graph from GrAF"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Translation Graphs from GrAF/XML files\n",
      "\n",
      "In this tutorial we will demonstrate how to extract a translation graph from data in digitized dictionaries. The translation graph connects entries in dioctionaries, via annotation for \"heads\" and \"translations\" within the dictionary. We will demonstrate how to visualize this data with a plotting library and hwo to export parts of the graph to JSON for interactive visualizations in the web.\n",
      "\n",
      "You can download this tutorial as [IPython notebook](http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html) here:\n",
      "\n",
      "https://github.com/cidles/graf-python/blob/master/examples/Translation%20Graph%20from%20GrAF.ipynb\n",
      "\n",
      "Or as a Python script here:\n",
      "\n",
      "https://github.com/cidles/graf-python/blob/master/examples/Translation%20Graph%20from%20GrAF.py"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data\n",
      "\n",
      "For this tutorial we will use data from the project \"[Quantitative Historical Linguistics](http://www.quanthistling.info/)\". The website of the project provides a ZIP package of GrAF/XML files for the printed sources that were digitized within the project:\n",
      "\n",
      "http://www.quanthistling.info/data/downloads/xml/data.zip\n",
      "\n",
      "The ZIP package contains several files encoded as described in the [ISO standard 24612](http://www.iso.org/iso/catalogue_detail.htm?csnumber=37326) \"Linguistic annotation framework (LAF)\". The QuantHistLing data contains dictionary and wordlist sources. Those were first tokenized into entries, for each entry you will find annotations for at least the head word(s) (\"head\" annotation) and translation(s) (\"translation\" annotation) in the case of dictionaries. We will only use the dictionaries of the \"Witotoan\" compoment in this tutorial. The ZIP package also contains a CSV file \"sources.csv\" that contains some information for each source, for example the languages as ISO codes, type of source, etc. Be aware that the ZIP package contains a filtered version of the sources: only entries that contain a Spanish stem that is included in the [Spanish swadesh list](http://en.wiktionary.org/wiki/Appendix:Spanish_Swadesh_list) are included in the download package.\n",
      "\n",
      "For a simple example how to parse on of the source please see here:\n",
      "\n",
      "http://graf-python.readthedocs.org/en/latest/Querying%20GrAF%20graphs.html\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## What are translation graphs?\n",
      "\n",
      "In our case, translation graphs are graphs that connect all spanish translation with every head word that we find for each translation in our sources. The idea is that spanish is some kind of interlingua in our case: if a string of a spanish translation in one source matches a string in another source this will only be '''one''' node in our graph. For the head words, this is not the case: matching strings in head words in different source are different nodes in the graph. This holds even if the different sources describe the same language, as different sources will use different orthographies. \n",
      "\n",
      "To fullfil that need, head words are internally represented as a string with two parts: the head word and its source. Both parts are seperated by a pipe symbol \"|\". For example, in a [DOT file](http://en.wikipedia.org/wiki/DOT_language) such a node looks like this:\n",
      "\n",
      "> \"\u00f3c\u00e1ji|thiesen1998\" [lang=boa, source=thiesen1998_25_339];\n",
      "\n",
      "The square brackets contain additional attributes here. These attributes are not part of the node's name, they contain just additonal information the user wants to store with the nodes.\n",
      "\n",
      "In comparison, a spanish translation looks like this:\n",
      "\n",
      "> \"vaca\" [lang=spa];\n",
      "\n",
      "There is no attribute \"source\" here, as this translation might occur in several sources. An edge connecting the two nodes looks like this:\n",
      "\n",
      "> \"vaca\" -- \"\u00f3c\u00e1ji|thiesen1998\";\n",
      "\n",
      "To handle such graphs our scripts use the [NetworkX Python library](http://networkx.lanl.gov/). It is kind of a standard in scientific graph computing with Python (remark: I started with the pygraph library, which has more or less the same API but by far not enough operations to compute with graphs later)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Requirements\n",
      "\n",
      "The following Python libraries are required to process the GrAF/XML files and create the translation graphs:\n",
      "\n",
      "* NetworkX: http://networkx.lanl.gov/\n",
      "* graf-python: https://github.com/cidles/graf-python\n",
      "* NLTK: http://nltk.org\n",
      "\n",
      "To visualize the graphs you have to install matplotlib:\n",
      "\n",
      "* matplotlib: http://matplotlib.org/\n",
      "\n",
      "When you installed all the libraries you are able to import the following modules:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "import codecs\n",
      "import re\n",
      "import glob\n",
      "\n",
      "import networkx\n",
      "import graf\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Get Witotoan sources\n",
      "\n",
      "Change to the directory where you extracted the ZIP archive that you downloaded from the QuantHistLing website:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir(\"c:/Users/Peter/Documents/Corpora/qlc\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we open the file \"sources.csv\" and read out all the sources that are part of the component \"Witotoan\" and that are dictionaries. We will store a list of those source in `witotoan_sources`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sources = csv.reader(open(\"sources.csv\", \"rU\"), delimiter=\"\\t\")\n",
      "witotoan_sources = list()\n",
      "for source in sources:\n",
      "    if source[5] == \"Witotoan\" and source[1] == \"dictionary\":\n",
      "        witotoan_sources.append(source[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## GrAF to NetworkX\n",
      "\n",
      "Next we define a helper function that transform a GrAF graph into a networkx graph. For this we traverse the graph by querying for all entries. For each entry we look for connected nodes that have \"head\" or \"translation\" annotation. All of those nodes that are Spanish are stored in the list `spa`. All non-Spanish annotations are stored in `others`. In the end the collected annotation are added to the new networkx graph, and each spanish node is connected to all the other nodes for each entry:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def graf_to_networkx(graf, source = None):\n",
      "    g = networkx.Graph()\n",
      "    for (node_id, node) in graf.nodes.items():\n",
      "        spa = list()\n",
      "        others = dict()\n",
      "        if node_id.endswith(\"entry\"):\n",
      "            for e in node.out_edges:\n",
      "                if e.annotations.get_first().label == \"head\" or e.annotations.get_first().label == \"translation\":\n",
      "                    # get lang\n",
      "                    for n in e.to_node.links[0][0].nodes:\n",
      "                        if n.annotations.get_first().label == \"iso-639-3\":\n",
      "                            if n.annotations.get_first().features.get_value(\"substring\") == \"spa\":\n",
      "                                spa.append(e.to_node.annotations.get_first().features.get_value(\"substring\"))\n",
      "                                break\n",
      "                            else:\n",
      "                                others[e.to_node.annotations.get_first().features.get_value(\"substring\")] = n.annotations.get_first().features.get_value(\"substring\")\n",
      "                                break\n",
      "        if len(spa) > 0:\n",
      "            for head in spa:\n",
      "                g.add_node(head, attr_dict={ \"lang\": \"spa\" })\n",
      "                for translation in others:\n",
      "                    g.add_node(u\"{0}|{1}\".format(translation, source), attr_dict={\n",
      "                        \"lang\": others[translation],\n",
      "                        \"source\": source \n",
      "                    })\n",
      "                    g.add_edge(head, u\"{0}|{1}\".format(translation, source))\n",
      "    return g"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Parse GrAF/XML files\n",
      "\n",
      "Now we parse all the XML files of the extracted ZIP package. For this we traverse through all the directories that have a name in `witotoan_sources'. The files we are looking for are the \"-dictinterpretation.xml\" files within each directory, as those contain the annotations for \"heads\" and \"translations\".\n",
      "\n",
      "First we create an empty list `graphs` that will later store all the networkx graphs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parser = graf.GraphParser()\n",
      "graphs = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we loop through all the Witotoan sources, parse the XML files and transform the graphs into networkx graph by calling the helper function that we defined above. We print a progress report within the loop, as parsing and transformation might take some time:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for d in witotoan_sources:\n",
      "    for f in glob.glob(os.path.join(d, \"dict-*-dictinterpretation.xml\")):\n",
      "        print(\"Parsing {0}...\".format(f))\n",
      "        graf_graph = parser.parse(f)\n",
      "        g = graf_to_networkx(graf_graph, d)\n",
      "        graphs.append(g)\n",
      "print(\"OK\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parsing thiesen1998\\dict-thiesen1998-25-339-dictinterpretation.xml...\n",
        "Parsing minor1987\\dict-minor1987-1-126-dictinterpretation.xml..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing minor1971\\dict-minor1971-3-74-dictinterpretation.xml..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing burtch1983\\dict-burtch1983-19-262-dictinterpretation.xml..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing leach1969\\dict-leach1969-67-161-dictinterpretation.xml..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing walton1997\\dict-walton1997-9-120-dictinterpretation.xml..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing preuss1994\\dict-preuss1994-797-912-dictinterpretation.xml..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing rivet1953\\dict-rivet1953-336-377-dictinterpretation.xml..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing griffiths2001\\dict-griffiths2001-79-199-dictinterpretation.xml..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "OK"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Merge all graphs\n",
      "\n",
      "Now we can merge all the individual graphsm for each source into one big graph. This will collapse all Spanish nodes and so connect the nodes that have a common Spanish translation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import copy\n",
      "combined_graph = copy.deepcopy(graphs[0])\n",
      "for gr in graphs[1:]:\n",
      "    for node in gr:\n",
      "        combined_graph.add_node(node, gr.node[node])\n",
      "    for n1, n2 in gr.edges_iter():\n",
      "        combined_graph.add_edge(n1, n2, gr.edge[n1][n2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We count the nodes in the graph and the [number of connected components](http://networkx.lanl.gov/reference/generated/networkx.algorithms.components.connected.number_connected_components.html#networkx.algorithms.components.connected.number_connected_components) to get an impression how the graph \"looks\". The number of nodes is much higher then the number of connected components, so we already have a lot of the nodes connected in groups, either as a consequence from being part of one dictionary entry or through the merge we did via the Spanish node:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(combined_graph.nodes())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "73022"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "networkx.algorithms.components.number_connected_components(combined_graph)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "17021"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Connect nodes with the same stem\n",
      "\n",
      "The next step is to connect spanish translations that contain the same stem. For this we first remove certain stop words from the translation (list of stopwords from NLTK). There are two cases then: just one word remains, or more than one word remains.\n",
      "\n",
      "We have to options now what to do with the latter: either they are not connected with anything at all (default behaviour), or each word is stemmed and the translation is connected with every other translation that contain the same stems. Right now this results in many connections that look not very useful. This should be done in a more intelligent way in the future (for example find heads of phrases in mulitword expression and only connect those; split the weight of the connections between all stems and work with weighted graphs from this step on; ...).\n",
      "\n",
      "To connect the spanish translations the script adds additional \"stem nodes\" to the graph. The name of these nodes consists of a spanish word stem plus a pipe symbol plus the string \"stem\". These nodes look like this in a dot file:\n",
      "\n",
      "> \"tom|stem\" [is_stem=True];\n",
      "\n",
      "The introduction of these nodes later facilites the output of translation matrixes, as you can just search for stems within the graph and only output direct neighbours with spanish translations. It would also be possible to directly connect the spanish translations if they have a matching stem, but then the graph traversal to find matching translations and their heads is a bit more complex later.\n",
      "\n",
      "First we create a stemmer object from the SpanishStemmer in NLTK:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.snowball import SpanishStemmer\n",
      "from nltk.corpus import swadesh\n",
      "\n",
      "stemmer = SpanishStemmer(True)\n",
      "\n",
      "def stemmed_swadesh_words(language):\n",
      "    stemmer = SpanishStemmer(True)\n",
      "    res = list()\n",
      "    for swadesh_entry in swadesh.words(language):\n",
      "        swadesh_entry = swadesh_entry.decode(\"utf-8\")\n",
      "        this_entry = list()\n",
      "        for w in swadesh_entry.split(\",\"):\n",
      "            w = w.strip()\n",
      "            stem = stemmer.stem(w)\n",
      "            this_entry.append(stem)\n",
      "        res.append(this_entry)\n",
      "    return res\n",
      "\n",
      "stemmed_swadesh = stemmed_swadesh_words(\"es\")\n",
      "stemmed_swadesh_flat = [ stem for stem_list in stemmed_swadesh for stem in stem_list ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We create the list of stopwords and encode them as unicode strings:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "combined_graph_stemmed = copy.deepcopy(combined_graph)\n",
      "stopwords = nltk.corpus.stopwords.words(\"spanish\")\n",
      "stopwords = [w.decode(\"utf-8\") for w in stopwords]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we loop through all the nodes of the merged graph and add the stem nodes to each Spanish node. If the node has only one word (after stopword removal) we will use the NLTK stemmer; otherwise we just leave the phrase as it is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "combined_graph_stemmed = copy.deepcopy(combined_graph)\n",
      "for node in combined_graph.nodes():\n",
      "    if \"lang\" in combined_graph.node[node] and combined_graph.node[node][\"lang\"] == \"spa\":\n",
      "        e = re.sub(\" ?\\([^)]\\)\", \"\", node)\n",
      "        e = e.strip()\n",
      "        stem = e\n",
      "        words = e.split(\" \")\n",
      "        if len(words) > 1:\n",
      "            words = [w for w in words if not w in stopwords or w == \"\"]\n",
      "        if len(words) == 1:\n",
      "            stem = stemmer.stem(words[0])\n",
      "            \n",
      "        stem = stem + \"|stem\"\n",
      "        combined_graph_stemmed.add_node(stem, is_stem=True)\n",
      "        combined_graph_stemmed.add_edge(stem, node)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again we can count the nodes and the number of connected components. We see that the number of connected components decreases, as more nodes are connected into groups now:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "networkx.algorithms.components.number_connected_components(combined_graph_stemmed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "13944"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(combined_graph_stemmed.nodes())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "100447"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Export the merged graph as DOT\n",
      "\n",
      "The graph may now be exported to the DOT format, to be used in other tools for graph analysis or visualization. For this we use a helper function from the [qlc library](https://github.com/pbouda/qlc):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from qlc.translationgraph import read, write\n",
      "#OUT = codecs.open(\"translation_graph_stemmed.dot\", \"w\", \"utf-8\")\n",
      "#OUT.write(write(combined_graph_stemmed))\n",
      "#OUT.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Extract a subgraph for the stem of \"comer\"\n",
      "\n",
      "As an example how to further process the graph we will extract the subgraph for the stem \"comer\" now. For this the graph is traversed again until the node \"com|stem\" is found. All the neighbours of this node are copied to a new graph. We will also remove the sources from the node strings to make the final visualization more readable:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "swadesh_graphs = list()\n",
      "for i, _ in enumerate(stemmed_swadesh):\n",
      "    swadesh_graphs.append(networkx.Graph())\n",
      "for node in combined_graph_stemmed:\n",
      "    if node.endswith(\"|stem\"):\n",
      "        (stem, _) = node.split(\"|\")\n",
      "        for i, stem_list in enumerate(stemmed_swadesh):\n",
      "            if stem in stem_list:\n",
      "                swadesh_graphs[i].add_node(node)\n",
      "                # spanish nodes\n",
      "                swadesh_graphs[i].add_node(\"spa\")\n",
      "                swadesh_graphs[i].add_edge(node, \"spa\")\n",
      "        \n",
      "                for sp in combined_graph_stemmed[node]:\n",
      "                    if \"lang\" in combined_graph_stemmed.node[sp] and combined_graph_stemmed.node[sp][\"lang\"] == \"spa\":\n",
      "                        swadesh_graphs[i].add_node(sp)\n",
      "                        swadesh_graphs[i].add_edge(\"spa\", sp)\n",
      "                \n",
      "                        for n in combined_graph_stemmed[sp]:\n",
      "                            if (\"lang\" in combined_graph_stemmed.node[n] and combined_graph_stemmed.node[n][\"lang\"] != \"spa\") and \\\n",
      "                                    (\"is_stem\" not in combined_graph_stemmed.node[n] or not combined_graph_stemmed.node[n][\"is_stem\"]):\n",
      "                                s, source = n.split(\"|\")\n",
      "                                lang = combined_graph_stemmed.node[n][\"lang\"]\n",
      "                                swadesh_graphs[i].add_node(lang)\n",
      "                                swadesh_graphs[i].add_edge(node, lang)\n",
      "                                swadesh_graphs[i].add_node(s, )\n",
      "                                swadesh_graphs[i].add_edge(lang, s, attr_dict={ \"data_source\": source })"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Export the subgraph as JSON data\n",
      "\n",
      "Another method to visualize the graph is the [D3 Javascript library](http://d3js.org/). For this we need to export the graph as JSON data that will be loaded by a HTML document. The networkx contains a `networkx.readwrite.json_graph` module that allows us to easily transform the graph into a JSON document. The JSON data structure can then be writte to a file with the help of the Python `json` module:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from networkx.readwrite import json_graph\n",
      "import json\n",
      "\n",
      "for i, g in enumerate(swadesh_graphs):\n",
      "    json_data = json_graph.node_link_data(g)\n",
      "    json.dump(json_data, codecs.open(\"swadesh_data_{0}.json\".format(i), \"w\", \"utf-8\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "json.dump(swadesh.words(\"es\"), codecs.open(\"swadesh_list.json\", \"w\", \"utf-8\"))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}