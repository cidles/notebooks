{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Use a Wikipedia corpus to extract semantic similarities\n",
      "\n",
      "In this tutorial we will use a Wikipedia corpus to extract semantically similar words for a given word. We will use a technique called \"Latent Semantic Analysis\" (LSA) that we apply onto a collocation matrix extracted from the [Bavarian Wikipedia](http://bar.wikipedia.org/wiki/Hoamseitn). I used the ideas from the paper of Widdows and Dorow (2002) \"[Visualisation Techniques for Analysing Meaning](http://www.puttypeg.net/papers/visualising-meaning.pdf)\" for this tutorial. You can read more about LSA and related technology in the book [Introduction to Information Retrieval](http://www.amazon.de/gp/product/0521865719/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&camp=1638&creative=6742&creativeASIN=0521865719&linkCode=as2&tag=jsusde-21). The data was prepared with the help of the [Wikipedia Extractor](http://medialab.di.unipi.it/wiki/Wikipedia_Extractor), which is probably the easiest way to extract a pure text corpus from the Wikipedia dumps. The dumps contain all kind of markup used in Wikipedia that we need to remove before we can process the data. This was already done and the data was transformed to [LAF/GrAF](http://www.iso.org/iso/catalogue_detail.htm?csnumber=37326). We will use a [Python GrAF parser](http://media.cidles.eu/poio/graf-python/) to read the data. For most of the calculation we will use [numpy](http://www.numpy.org/), [scipy](http://scipy.org/) and [sparsesvd](https://pypi.python.org/pypi/sparsesvd). Finally, [matplotlib](http://matplotlib.org/) will be used to visualize the space of semantic similarities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Prerequisites\n",
      "\n",
      "You need to install the following Python libraries in order to be able to execute this notebook. The easiest way to install them is via `easy_install`. If you are on Windows you can download setup packages for numpy and scipy [here](http://www.lfd.uci.edu/~gohlke/pythonlibs/).\n",
      "\n",
      "* graf-python: http://media.cidles.eu/poio/graf-python/\n",
      "* requests: http://docs.python-requests.org/en/latest/\n",
      "* numpy: http://www.numpy.org/\n",
      "* scipy: http://www.scipy.org/\n",
      "* sparsesvd: https://pypi.python.org/pypi/sparsesvd/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Change directory and imports\n",
      "\n",
      "First we change the directory to somewehere where we can download and extract the data. If you want to download and extract to the current directory you can just skip this step:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd \"h:\\ProjectsWin\\git-github\\poio-corpus\\build\\\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "h:\\ProjectsWin\\git-github\\poio-corpus\\build\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nex, We import all the Python modules that we need:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import io\n",
      "import math\n",
      "import codecs\n",
      "import zipfile\n",
      "\n",
      "import requests\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.spatial\n",
      "import scipy.sparse\n",
      "import scipy.linalg\n",
      "from sparsesvd import sparsesvd\n",
      "import graf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Download stopwords and stop characters\n",
      "\n",
      "A list of Bavarian stopwords was already compiled and is available for download. The next block of code download this list of stopwords and stores it in a variable `stopwords`. We will also download a list of characters that we want to ignore in Bavarian words and store it in the variable `ignorechars`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = requests.get(\"https://www.poio.eu/static/stopwords/bar.txt\")\n",
      "stopwords = r.content.decode(\"utf-8\").split()\n",
      "r = requests.get(\"https://www.poio.eu/static/ignorechars/bar.txt\")\n",
      "ignorechars = r.content.decode(\"utf-8\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Download, extract and parse the corpus\n",
      "\n",
      "In the next step we will download and extract the corpus. The corpus is pre-compiled from Wikipedia dumps and was converted to a set of GrAF files. To parse the corpus we will use the library graf-python (see \"Prerequisites\" above). We will store each document of the Wikipedia as a Unicode string in the list `documents`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = requests.get(\"https://www.poio.eu/static/corpus/barwiki-20130813.zip\")\n",
      "with open(\"barwiki-20130813.zip\", \"wb\") as f:\n",
      "    f.write(r.content)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = zipfile.ZipFile(\"barwiki-20130813.zip\")\n",
      "z.extractall()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gp = graf.GraphParser()\n",
      "g = gp.parse(\"barwiki-20130813.hdr\")\n",
      "\n",
      "text = codecs.open(\"barwiki-20130813.txt\", \"r\", \"utf-8\")\n",
      "txt = text.read()\n",
      "text.close()\n",
      "\n",
      "documents = list()\n",
      "for n in g.nodes:\n",
      "    if n.id.startswith(\"doc..\") and len(n.links) > 0 and len(n.links[0]) > 0:\n",
      "        doc = txt[n.links[0][0].start:n.links[0][0].end]\n",
      "        documents.append(doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Build a dict for words and doc ids"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re_ignore_chars = re.compile(u\"[{0}]\".format(ignorechars))\n",
      "def _words_for_document(doc):\n",
      "    words = doc.split()\n",
      "    words2 = list()\n",
      "\n",
      "    for w in words:\n",
      "        w = re_ignore_chars.sub(\"\", w.lower())\n",
      "\n",
      "        if not w or w in stopwords:\n",
      "            continue\n",
      "        \n",
      "        words2.append(w)\n",
      "\n",
      "    return words2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wdict = {}\n",
      "for i, d in enumerate(documents):\n",
      "    for w in _words_for_document(d):\n",
      "        if w in wdict:\n",
      "            wdict[w].append(i)\n",
      "        else:\n",
      "            wdict[w] = [i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pre-process"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Which 1000 words occur most often?\n",
      "top_words = [k for k in sorted(wdict, key=lambda k: len(wdict[k]), reverse=True)][:1000]\n",
      "# get all words that appear at least 3 times and sort them\n",
      "keys = [k for k in wdict.keys() if len(wdict[k]) > 2]\n",
      "keys.sort()\n",
      "keys_indices = { w: i for i, w in enumerate(keys) }\n",
      "# create and empty count matrix\n",
      "A = np.zeros([len(keys), len(top_words)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Process"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for d in documents:\n",
      "    words = _words_for_document(d)\n",
      "    len_words = len(words) - 1\n",
      "    for i, w in enumerate(words):\n",
      "        if w not in keys_indices:\n",
      "            continue\n",
      "        start = i - 15\n",
      "        if i < 0:\n",
      "            start = 0\n",
      "        end = len_words\n",
      "        if end > i + 15:\n",
      "            end = i + 15\n",
      "        for j, t in enumerate(top_words):\n",
      "            if w == t:\n",
      "                continue\n",
      "            if t in words[start:end]:\n",
      "                A[keys_indices[w],j] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Normalize"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_per_top = np.sum(A, axis=0)\n",
      "tops_per_word = np.sum(np.asarray(A > 0, 'i'), axis=1)\n",
      "rows, cols = A.shape\n",
      "for i in range(rows):\n",
      "    for j in range(cols):\n",
      "        if words_per_top[j] == 0 or tops_per_word[i] == 0:\n",
      "            A[i,j] = 0\n",
      "        else:\n",
      "            A[i,j] = (A[i,j] / words_per_top[j]) * math.log(float(cols) / tops_per_word[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tops_per_word[keys_indices['bia']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "311"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SVD calculation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#U, S, Vt = scipy.sparse.linalg.svds(A, 100)\n",
      "s_A = scipy.sparse.csc_matrix(A)\n",
      "ut, s, vt = sparsesvd(s_A, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out = open(\"bar-ut.bin\", \"wb\")\n",
      "np.save(out, ut)\n",
      "out.close()\n",
      "\n",
      "out = open(\"bar-s.bin\", \"wb\")\n",
      "np.save(out, s)\n",
      "out.close()\n",
      "\n",
      "out = open(\"bar-vt.bin\", \"wb\")\n",
      "np.save(out, vt)\n",
      "out.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "with open(\"bar-indices.pickle\", \"wb\") as f:\n",
      "    pickle.dump(keys_indices, f, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(s)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import scipy.linalg\n",
      "#reconstructed_matrix = np.dot(np.dot(U, scipy.linalg.diagsvd(S,len(S),len(S))), Vt)\n",
      "reconstructed_matrix = np.dot(ut.T, np.dot(np.diag(s), vt))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree = scipy.spatial.cKDTree(reconstructed_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neighbours = tree.query(reconstructed_matrix[keys_indices[u\"bia\"]], k=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subset = reconstructed_matrix[neighbours[1]]\n",
      "words = [keys[i] for i in neighbours[1]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tempU, tempS, tempVt = scipy.linalg.svd(subset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(tempS)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coords = tempU[:,1:3]\n",
      "plt.figure(1, figsize=(16,12))\n",
      "plt.plot(tempU[:,1], tempU[:,2], marker=\"o\", linestyle=\"None\")\n",
      "for label, x, y in zip(words, tempU[:,1], tempU[:,2]):\n",
      "    plt.annotate(\n",
      "        label, \n",
      "        xy = (x, y), xytext = (-5, 5),\n",
      "        textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
      "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}