{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Use a Wikipedia corpus to extract semantic similarities\n",
      "\n",
      "In this tutorial we will use a Wikipedia corpus to extract semantically similar words for a given word. We will use a technique called \"Latent Semantic Analysis\" (LSA) that we apply onto a collocation matrix extracted from the [Bavarian Wikipedia](http://bar.wikipedia.org/wiki/Hoamseitn). I used the ideas from the paper of Widdows and Dorow (2002) \"[Visualisation Techniques for Analysing Meaning](http://www.puttypeg.net/papers/visualising-meaning.pdf)\" for this tutorial. The data was prepared with the help of the [Wikipedia Extractor](http://medialab.di.unipi.it/wiki/Wikipedia_Extractor), which is probably the easiest way to extract a pure text corpus from the Wikipedia dumps. The dump contains all kind of markup used in Wikipedia that we need to remove before we can process the data. This was already done and the data was transformed to [LAF/GrAF](http://www.iso.org/iso/catalogue_detail.htm?csnumber=37326). We will use a Python GrAF parser to read the data. For most of the calculation we will use [numpy](http://www.numpy.org/) and [scipy](http://scipy.org/). Finally, [matplotlib](http://matplotlib.org/) will be used to visualize the space of semantic similarities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Prerequisites"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd \"c:\\Users\\Peter\\Projects\\git-github\\poio-corpus\\public\\\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import io\n",
      "import math\n",
      "import codecs\n",
      "\n",
      "import requests\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.sparse.linalg\n",
      "import graf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = requests.get(\"https://s3.amazonaws.com/wikipedia-graf/bar/stopwords.txt\")\n",
      "stopwords = r.content.decode(\"utf-8\").split()\n",
      "r = requests.get(\"https://s3.amazonaws.com/wikipedia-graf/bar/ignorechars.txt\")\n",
      "ignorechars = r.content.decode(\"utf-8\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = requests.get(\"https://s3.amazonaws.com/wikipedia-graf/bar/barwiki.zip\")\n",
      "with open(\"barwiki.zip\", \"wb\") as f:\n",
      "    f.write(r.content)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import zipfile\n",
      "z = zipfile.ZipFile(\"barwiki-20130725.zip\")\n",
      "z.extractall()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gp = graf.GraphParser()\n",
      "g = gp.parse(\"barwiki-20130725.hdr\")\n",
      "\n",
      "text = codecs.open(\"barwiki-20130725.txt\", \"r\", \"utf-8\")\n",
      "txt = text.read()\n",
      "text.close()\n",
      "\n",
      "documents = list()\n",
      "for n in g.nodes:\n",
      "    if n.id.startswith(\"doc..\") and len(n.links) > 0 and len(n.links[0]) > 0:\n",
      "        doc = txt[n.links[0][0].start:n.links[0][0].end]\n",
      "        documents.append(doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Build a dict for words and doc ids"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re_ignore_chars = re.compile(u\"[{0}]\".format(ignorechars))\n",
      "def _words_for_document(doc):\n",
      "    words = doc.split()\n",
      "    words2 = list()\n",
      "\n",
      "    for w in words:\n",
      "        w = re_ignore_chars.sub(\"\", w.lower())\n",
      "\n",
      "        if not w or w in stopwords:\n",
      "            continue\n",
      "        \n",
      "        words2.append(w)\n",
      "\n",
      "    return words2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wdict = {}\n",
      "for i, d in enumerate(documents):\n",
      "    for w in _words_for_document(d):\n",
      "        if w in wdict:\n",
      "            wdict[w].append(i)\n",
      "        else:\n",
      "            wdict[w] = [i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pre-process"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Which 1000 words occur most often?\n",
      "top_words = [k for k in sorted(wdict, key=lambda k: len(wdict[k]), reverse=True)][:1000]\n",
      "# get all words that appear at least 3 times and sort them\n",
      "keys = [k for k in wdict.keys() if len(wdict[k]) > 2]\n",
      "keys.sort()\n",
      "keys_indices = { w: i for i, w in enumerate(keys) }\n",
      "# create and empty count matrix\n",
      "A = np.zeros([len(keys), len(top_words)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Process"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for d in documents:\n",
      "    words = _words_for_document(d)\n",
      "    len_words = len(words) - 1\n",
      "    for i, w in enumerate(words):\n",
      "        if w not in keys_indices:\n",
      "            continue\n",
      "        start = i - 15\n",
      "        if i < 0:\n",
      "            start = 0\n",
      "        end = len_words\n",
      "        if end > i + 15:\n",
      "            end = i + 15\n",
      "        for j, t in enumerate(top_words):\n",
      "            if w == t:\n",
      "                continue\n",
      "            if t in words[start:end]:\n",
      "                A[keys_indices[w],j] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Normalize"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_per_top = np.sum(A, axis=0)\n",
      "tops_per_word = np.sum(np.asarray(A > 0, 'i'), axis=1)\n",
      "rows, cols = A.shape\n",
      "for i in range(rows):\n",
      "    for j in range(cols):\n",
      "        if words_per_top[j] == 0 or tops_per_word[i] == 0:\n",
      "            A[i,j] = 0\n",
      "        else:\n",
      "            A[i,j] = (A[i,j] / words_per_top[j]) * math.log(float(cols) / tops_per_word[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tops_per_word[keys_indices['bia']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "304"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SVD calculation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#U, S, Vt = scipy.sparse.linalg.svds(A, 100)\n",
      "from sparsesvd import sparsesvd\n",
      "s_A = scipy.sparse.csc_matrix(A)\n",
      "ut, s, vt = sparsesvd(s_A, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out = open(\"bar-ut.bin\", \"wb\")\n",
      "np.save(out, ut)\n",
      "out.close()\n",
      "\n",
      "out = open(\"bar-s.bin\", \"wb\")\n",
      "np.save(out, s)\n",
      "out.close()\n",
      "\n",
      "out = open(\"bar-vt.bin\", \"wb\")\n",
      "np.save(out, vt)\n",
      "out.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "with open(\"bar-indices.pickle\", \"wb\") as f:\n",
      "    pickle.dump(keys_indices, f, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(s)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import scipy.linalg\n",
      "#reconstructed_matrix = np.dot(np.dot(U, scipy.linalg.diagsvd(S,len(S),len(S))), Vt)\n",
      "reconstructed_matrix = np.dot(ut.T, np.dot(np.diag(s), vt))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.spatial\n",
      "tree = scipy.spatial.cKDTree(reconstructed_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out = open(\"matrix.bin\", \"wb\")\n",
      "np.save(out, scipy.sparse.csc_matrix(reconstructed_matrix))\n",
      "out.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "MemoryError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-56-d4b81a940c75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"matrix.bin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Python33\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     64\u001b[0m                         self.format)\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# Read matrix dimensions given, if any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Python33\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mMemoryError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "out = open(\"tree.pickle\", \"wb\")\n",
      "pickle.dump(tree, out)\n",
      "out.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neighbours = tree.query(reconstructed_matrix[keys_indices[u\"bia\"]], k=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subset = reconstructed_matrix[neighbours[1]]\n",
      "words = [keys[i] for i in neighbours[1]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tempU, tempS, tempVt = scipy.linalg.svd(subset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(tempS)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coords = tempU[:,1:3]\n",
      "plt.figure(1, figsize=(16,12))\n",
      "plt.plot(tempU[:,1], tempU[:,2], marker=\"o\", linestyle=\"None\")\n",
      "for label, x, y in zip(words, tempU[:,1], tempU[:,2]):\n",
      "    plt.annotate(\n",
      "        label, \n",
      "        xy = (x, y), xytext = (-5, 5),\n",
      "        textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
      "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    }
   ],
   "metadata": {}
  }
 ]
}